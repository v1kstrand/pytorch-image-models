{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba783e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb6d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention import sdpa_kernel, SDPBackend\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "B, N, D, H, d =  32,  8,  384, 6,  64\n",
    "qkv = torch.randn(B, N, D * 3) \n",
    "qkv = qkv.reshape(B, N, 3, H, d).permute(2, 0, 3, 1, 4)\n",
    "q, k, v = qkv.unbind(0)\n",
    "\n",
    "q.is_contiguous()        # False (view)\n",
    "q.stride(-1) == 1        # True  ✅ last-dim contiguous\n",
    "\n",
    "with sdpa_kernel([SDPBackend.FLASH_ATTENTION]):\n",
    "    F.scaled_dot_product_attention(q, k, v)  # raises if layout not supported by Fla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f8b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, torch\n",
    "os.environ[\"COMET_API_KEY\"] = \"R7OuT6FolA02VmQRI82xDN48O\"\n",
    "sys.path.insert(0, \"/notebooks/pytorch-image-models\")  # your fork\n",
    "import train as timm_train\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(False)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "dynamo_config = torch._dynamo.config\n",
    "dynamo_config.compiled_autograd = True\n",
    "dynamo_config.capture_scalar_outputs = False\n",
    "dynamo_config.cache_size_limit = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6cc66e",
   "metadata": {},
   "source": [
    "# Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2dc6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "GROUP_NM_SWEEP = [4]\n",
    "NUM_STAGES_SWEEP = [7]\n",
    "NUM_WARPS_SWEEP = [8]\n",
    "KEY_CACHE = [\"BATCH_SIZE\", \"NUM_HEADS\", \"SEQ_LEN\", \"HEAD_DIM\"]\n",
    "\n",
    "def _sdpa_comp_dtype(x: torch.Tensor) -> torch.dtype:\n",
    "    return torch.get_autocast_dtype('cuda') if torch.is_autocast_enabled() else x.dtype\n",
    "\n",
    "def _triton_compute_dtype(dtype: torch.dtype):\n",
    "    if dtype is torch.float16:\n",
    "        return tl.float16\n",
    "    if dtype is torch.bfloat16:\n",
    "        return tl.bfloat16\n",
    "    if dtype is torch.float32:\n",
    "        return tl.float32\n",
    "    raise ValueError(f\"Unsupported compute dtype for Triton SDPA: {dtype}\")\n",
    "\n",
    "@triton.jit\n",
    "def _attn_fwd_inner(\n",
    "    O_block, l_i, m_i, Q_block,\n",
    "    K_block_ptr, V_block_ptr,\n",
    "    softmax_scale: tl.constexpr, BLOCK_KV: tl.constexpr,\n",
    "    SEQ_LEN: tl.constexpr, DTYPE: tl.constexpr,\n",
    "):\n",
    "    s = tl.full([1], softmax_scale, dtype=DTYPE)\n",
    "    Q_block = Q_block * s\n",
    "    offs_kv = tl.arange(0, BLOCK_KV)\n",
    "    for start_kv in range(0, SEQ_LEN, BLOCK_KV):\n",
    "        K_block = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "        S = tl.dot(Q_block, K_block) \n",
    "\n",
    "        kv_idx  = start_kv + offs_kv\n",
    "        kv_valid = kv_idx < SEQ_LEN\n",
    "        S = tl.where(kv_valid[None, :], S, -float(\"inf\"))\n",
    "\n",
    "        m_ij = tl.maximum(m_i, tl.max(S, axis=1))\n",
    "        P_block = tl.exp(S - m_ij[:, None])\n",
    "        l_ij = tl.sum(P_block, axis=1)\n",
    "\n",
    "        alpha = tl.exp(m_i - m_ij)\n",
    "        l_i = l_i * alpha + l_ij\n",
    "\n",
    "        V_block = tl.load(V_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "        P_block = P_block.to(DTYPE)\n",
    "        V_block = V_block.to(DTYPE)\n",
    "\n",
    "        O_block = O_block * alpha[:, None]\n",
    "        O_block = tl.dot(P_block, V_block, O_block)\n",
    "\n",
    "        m_i = m_ij\n",
    "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_KV, 0))\n",
    "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_KV))\n",
    "    \n",
    "    O_block = O_block / l_i[:, None]\n",
    "    return O_block, l_i, m_i\n",
    "\n",
    "@triton.autotune(\n",
    "    [\n",
    "        triton.Config(\n",
    "            {\"BLOCK_Q\": BLOCK_Q, \"BLOCK_KV\": BLOCK_KV, \"GROUP_M\": GROUP_M},\n",
    "            num_stages=num_stages,\n",
    "            num_warps=num_warps,\n",
    "        )\n",
    "        for BLOCK_Q in [64, 128]\n",
    "        for BLOCK_KV in [32, 64]\n",
    "        for GROUP_M in GROUP_NM_SWEEP\n",
    "        for num_stages in NUM_STAGES_SWEEP\n",
    "        for num_warps in NUM_WARPS_SWEEP\n",
    "    ],\n",
    "    key=KEY_CACHE,\n",
    ")\n",
    "@triton.jit\n",
    "def _attn_fwd(\n",
    "    Q, K, V, M, O,\n",
    "    # Q strides\n",
    "    sqb, sqh, sqs, sqd,\n",
    "    # K strides\n",
    "    skb, skh, sks, skd,\n",
    "    # V strides\n",
    "    svb, svh, svs, svd,\n",
    "    # O strides\n",
    "    sob, soh, sos, sod,\n",
    "    # dK strides\n",
    "    NUM_HEADS: tl.constexpr, SEQ_LEN: tl.constexpr, HEAD_DIM: tl.constexpr,\n",
    "    softmax_scale:tl.constexpr, BLOCK_Q: tl.constexpr, BLOCK_KV: tl.constexpr, \n",
    "    DTYPE: tl.constexpr, GROUP_M: tl.constexpr,\n",
    "):\n",
    "    tl.static_assert(BLOCK_KV <= HEAD_DIM)\n",
    "\n",
    "    # --- program ids ---\n",
    "    pid_m  = tl.program_id(0)\n",
    "    pid_bh = tl.program_id(1)\n",
    "\n",
    "    num_tiles_m   = tl.cdiv(SEQ_LEN, BLOCK_Q)                       # ceil_div\n",
    "    group_id      = pid_m // GROUP_M\n",
    "    tiles_in_this = tl.minimum(GROUP_M, num_tiles_m - group_id*GROUP_M)\n",
    "\n",
    "    m_in_grp      = pid_m - group_id*GROUP_M                        # 0..GROUP_M-1\n",
    "    m_in_grp_eff  = m_in_grp % tiles_in_this                        # clamp to tail size\n",
    "    rot           = pid_bh % tiles_in_this\n",
    "    m_swizzled    = group_id*GROUP_M + ((m_in_grp_eff + rot) % tiles_in_this)\n",
    "\n",
    "    start_q       = m_swizzled * BLOCK_Q\n",
    "    if start_q >= SEQ_LEN:\n",
    "        return\n",
    "\n",
    "    b = pid_bh // NUM_HEADS\n",
    "    h  = pid_bh %  NUM_HEADS\n",
    "\n",
    "    off_bh_k  = (b * skb   + h * skh  ).to(tl.int64)\n",
    "    off_bh_v  = (b * svb   + h * svh  ).to(tl.int64)\n",
    "    off_bh_q  = (b * sqb   + h * sqh  ).to(tl.int64)\n",
    "    off_bh_o = (b * sob   + h * soh  ).to(tl.int64)\n",
    "    \n",
    "    # --- block pointers ---\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        Q + off_bh_q, (SEQ_LEN, HEAD_DIM), (sqs, sqd), (start_q, 0), (BLOCK_Q, HEAD_DIM), (1, 0)\n",
    "    )\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        V + off_bh_v, (SEQ_LEN, HEAD_DIM), (svs, svd), (0, 0), (BLOCK_KV, HEAD_DIM), (1, 0)\n",
    "    )\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        K + off_bh_k, (HEAD_DIM, SEQ_LEN), (skd, sks), (0, 0), (HEAD_DIM, BLOCK_KV), (0, 1)\n",
    "    )\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        O + off_bh_o, (SEQ_LEN, HEAD_DIM), (sos, sod), (start_q, 0), (BLOCK_Q, HEAD_DIM), (1, 0)\n",
    "    )\n",
    "\n",
    "    # --- per-row running stats + output tile ---\n",
    "    m_i = tl.full((BLOCK_Q,), -float(\"inf\"), dtype=tl.float32)\n",
    "    l_i = tl.full((BLOCK_Q,),  1,          dtype=tl.float32)\n",
    "    O_block = tl.zeros([BLOCK_Q, HEAD_DIM], dtype=tl.float32)\n",
    "    Q_block = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "\n",
    "    # --- inner loop over KV tiles (online softmax) ---\n",
    "    O_block, l_i, m_i = _attn_fwd_inner(\n",
    "        O_block, l_i, m_i, Q_block,\n",
    "        K_block_ptr, V_block_ptr, softmax_scale,\n",
    "        BLOCK_KV, SEQ_LEN, DTYPE\n",
    "    )\n",
    "\n",
    "    # --- write back: store log-sum-exp (for bwd) and O ---\n",
    "    offs_q  = start_q + tl.arange(0, BLOCK_Q)\n",
    "    m_i += tl.math.log(l_i + 1e-20)\n",
    "    m_ptrs = M + pid_bh * SEQ_LEN + offs_q\n",
    "    tl.store(m_ptrs, m_i, mask=offs_q < SEQ_LEN)\n",
    "    tl.store(O_block_ptr, O_block.to(O.type.element_ty), boundary_check=(0, 1))\n",
    "\n",
    "@triton.autotune(\n",
    "    [triton.Config({\"BLOCK_Q\": bq}, num_stages=ns, num_warps=nw)\n",
    "     for bq in [32, 64, 128]\n",
    "     for ns in NUM_STAGES_SWEEP\n",
    "     for nw in NUM_WARPS_SWEEP],\n",
    "    key=KEY_CACHE,\n",
    ")\n",
    "@triton.jit\n",
    "def _attn_bwd_preprocess(\n",
    "    O, dO, D,\n",
    "    sOb, sOh, sOs, sOd,          # O strides\n",
    "    sdb, sdh, sds, sdd,          # dO strides\n",
    "    NUM_HEADS: tl.constexpr, SEQ_LEN: tl.constexpr,\n",
    "    BLOCK_Q: tl.constexpr, HEAD_DIM: tl.constexpr,\n",
    "):\n",
    "    pid_q  = tl.program_id(0)                          # Q-tile id\n",
    "    pid_bh = tl.program_id(1)                          # packed (batch, head)\n",
    "    start_q = pid_q * BLOCK_Q\n",
    "    if start_q >= SEQ_LEN:\n",
    "        return\n",
    "\n",
    "    b = pid_bh // NUM_HEADS\n",
    "    h = pid_bh %  NUM_HEADS\n",
    "    off_bh_O  = (b * sOb  + h * sOh ).to(tl.int64)\n",
    "    off_bh_dO = (b * sdb  + h * sdh ).to(tl.int64)\n",
    "\n",
    "    # use block_ptr so arbitrary strides are OK\n",
    "    O_blk = tl.make_block_ptr(\n",
    "        O + off_bh_O, (SEQ_LEN, HEAD_DIM), (sOs, sOd),\n",
    "        (start_q, 0), (BLOCK_Q, HEAD_DIM), (1, 0)\n",
    "    )\n",
    "    dO_blk = tl.make_block_ptr(\n",
    "        dO + off_bh_dO, (SEQ_LEN, HEAD_DIM), (sds, sdd),\n",
    "        (start_q, 0), (BLOCK_Q, HEAD_DIM), (1, 0)\n",
    "    )\n",
    "\n",
    "    O_block  = tl.load(O_blk,  boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)\n",
    "    dO_block = tl.load(dO_blk, boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)\n",
    "    D_block  = tl.sum(dO_block * O_block, axis=1)\n",
    "\n",
    "    offs_q = start_q + tl.arange(0, BLOCK_Q)\n",
    "    tl.store(D + pid_bh * SEQ_LEN + offs_q, D_block, mask=offs_q < SEQ_LEN)\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    [\n",
    "        triton.Config(\n",
    "            {\"BLOCK_Q\": BLOCK_Q, \"BLOCK_KV\": BLOCK_KV, \"GROUP_N\": GROUP_N},\n",
    "            num_stages=num_stages,\n",
    "            num_warps=num_warps,\n",
    "        )\n",
    "        for BLOCK_Q in [32, 64]\n",
    "        for BLOCK_KV in [64, 128]\n",
    "        for GROUP_N in GROUP_NM_SWEEP\n",
    "        for num_stages in NUM_STAGES_SWEEP\n",
    "        for num_warps in NUM_WARPS_SWEEP\n",
    "    ],\n",
    "    key=KEY_CACHE,\n",
    ")\n",
    "@triton.jit\n",
    "def _attn_bwd_dk_dv(\n",
    "    Q, K, V, dO, dK, dV, M, D,\n",
    "    # Q strides\n",
    "    sqb, sqh, sqs, sqd,\n",
    "    # K strides\n",
    "    skb, skh, sks, skd,\n",
    "    # V strides\n",
    "    svb, svh, svs, svd,\n",
    "    # dO strides\n",
    "    sob, soh, sos, sod,\n",
    "    # dK strides\n",
    "    s_dkb, s_dkh, s_dks, s_dkd,\n",
    "    # dV strides\n",
    "    s_dvb, s_dvh, s_dvs, s_dvd,\n",
    "    NUM_HEADS: tl.constexpr, SEQ_LEN: tl.constexpr,\n",
    "    BLOCK_Q: tl.constexpr, BLOCK_KV: tl.constexpr, softmax_scale: tl.constexpr,\n",
    "    HEAD_DIM: tl.constexpr, DTYPE: tl.constexpr, GROUP_N: tl.constexpr\n",
    "):\n",
    "    # --- program ids ---\n",
    "    pid_kv = tl.program_id(0)                 # which KV block\n",
    "    pid_bh = tl.program_id(1)                 # packed (batch, head)\n",
    "    b = pid_bh // NUM_HEADS\n",
    "    h = pid_bh %  NUM_HEADS\n",
    "\n",
    "    # --- base offsets for this (batch, head) slice ---\n",
    "    off_bh_seq = (pid_bh * SEQ_LEN).to(tl.int64)\n",
    "    M  += off_bh_seq\n",
    "    D  += off_bh_seq\n",
    "\n",
    "    num_tiles_kv = tl.cdiv(SEQ_LEN, BLOCK_KV)\n",
    "    group_id     = pid_kv // GROUP_N\n",
    "    group_start  = group_id * GROUP_N\n",
    "    if group_start >= num_tiles_kv:\n",
    "        return\n",
    "    \n",
    "    tiles_in_this = tl.minimum(GROUP_N, num_tiles_kv - group_start)\n",
    "    kv_in_grp     = pid_kv - group_start\n",
    "    kv_eff        = kv_in_grp % tiles_in_this\n",
    "    rot           = pid_bh % tiles_in_this\n",
    "    kv_tile_id    = group_start + ((kv_eff + rot) % tiles_in_this)\n",
    "\n",
    "    start_kv = kv_tile_id * BLOCK_KV\n",
    "    if start_kv >= SEQ_LEN:\n",
    "        return\n",
    "\n",
    "    off_bh_k  = (b * skb   + h * skh  ).to(tl.int64)\n",
    "    off_bh_v  = (b * svb   + h * svh  ).to(tl.int64)\n",
    "    off_bh_dk = (b * s_dkb + h * s_dkh).to(tl.int64)\n",
    "    off_bh_dv = (b * s_dvb + h * s_dvh).to(tl.int64)\n",
    "    off_bh_q  = (b * sqb   + h * sqh  ).to(tl.int64)\n",
    "    off_bh_do = (b * sob   + h * soh  ).to(tl.int64)\n",
    "    \n",
    "    K_blk = tl.make_block_ptr( \n",
    "        K + off_bh_k, (SEQ_LEN, HEAD_DIM), (sks, skd),(start_kv, 0),(BLOCK_KV, HEAD_DIM),(1, 0)\n",
    "    ) # base,        shape,               strides,                  offsets,       block_shape,          order\n",
    "    V_blk = tl.make_block_ptr( \n",
    "        V + off_bh_v,(SEQ_LEN, HEAD_DIM),(svs, svd),(start_kv, 0),(BLOCK_KV, HEAD_DIM),(1, 0)\n",
    "    )\n",
    "    dK_blk = tl.make_block_ptr( \n",
    "        dK + off_bh_dk, (SEQ_LEN, HEAD_DIM), (s_dks, s_dkd), (start_kv, 0), (BLOCK_KV, HEAD_DIM), (1, 0)\n",
    "    )\n",
    "    dV_blk = tl.make_block_ptr( \n",
    "        dV + off_bh_dv,(SEQ_LEN, HEAD_DIM),(s_dvs, s_dvd),(start_kv, 0),(BLOCK_KV, HEAD_DIM),(1, 0)\n",
    "    )\n",
    "    Q_T_blk = tl.make_block_ptr( \n",
    "        Q + off_bh_q,(HEAD_DIM, SEQ_LEN),(sqd, sqs),(0, 0),(HEAD_DIM, BLOCK_Q),(0, 1)\n",
    "    )\n",
    "    dO_blk = tl.make_block_ptr( \n",
    "        dO + off_bh_do,(SEQ_LEN, HEAD_DIM),(sos, sod),(0, 0),(BLOCK_Q, HEAD_DIM),(1, 0)\n",
    "    )\n",
    "\n",
    "    dV_acc = tl.zeros((BLOCK_KV, HEAD_DIM), dtype=tl.float32)\n",
    "    dK_acc = tl.zeros((BLOCK_KV, HEAD_DIM), dtype=tl.float32)\n",
    "    s = tl.full([1], softmax_scale, dtype=DTYPE)\n",
    "    K_block = tl.load(K_blk, boundary_check=(0, 1), padding_option=\"zero\").to(DTYPE) * s\n",
    "    V_block = tl.load(V_blk, boundary_check=(0, 1), padding_option=\"zero\").to(DTYPE)\n",
    "    offs_kv  = start_kv + tl.arange(0, BLOCK_KV)\n",
    "    \n",
    "    # Loop over Q tiles\n",
    "    num_steps = tl.cdiv(SEQ_LEN, BLOCK_Q)\n",
    "    for qi in range(0, num_steps):\n",
    "        qT_block = tl.load(Q_T_blk, boundary_check=(0, 1), padding_option=\"zero\").to(DTYPE)\n",
    "        dO_block = tl.load(dO_blk, boundary_check=(0, 1), padding_option=\"zero\").to(DTYPE)\n",
    "        \n",
    "        start_q = qi * BLOCK_Q\n",
    "        offs_q  = start_q + tl.arange(0, BLOCK_Q)\n",
    "        m  = tl.load(M + offs_q, mask=offs_q < SEQ_LEN, other=0.0).to(tl.float32)\n",
    "        Di = tl.load(D + offs_q, mask=offs_q < SEQ_LEN, other=0.0).to(tl.float32)\n",
    "\n",
    "        QK_T = tl.dot(K_block, qT_block) \n",
    "        kv_valid = offs_kv < SEQ_LEN\n",
    "        QK_T = tl.where(kv_valid[:, None], QK_T, -float(\"inf\"))\n",
    "        P_T = tl.exp(QK_T.to(tl.float32) - m[None, :])\n",
    "\n",
    "        # --- dV += Pᵀ @ dO  (match operand dtypes) ---\n",
    "        dV_acc += tl.dot(P_T.to(DTYPE), dO_block)\n",
    "\n",
    "        # --- dpᵀ = V @ dOᵀ, then dSᵀ = Pᵀ * (dpᵀ - Di) ---\n",
    "        dpT = tl.dot(V_block, tl.trans(dO_block)).to(tl.float32)\n",
    "        dS_T = (P_T * (dpT - Di[None, :])).to(DTYPE)\n",
    "        dK_acc = tl.dot(dS_T, tl.trans(qT_block), dK_acc)\n",
    "\n",
    "        Q_T_blk = tl.advance(Q_T_blk, (0, BLOCK_Q))\n",
    "        dO_blk = tl.advance(dO_blk, (BLOCK_Q, 0))\n",
    "\n",
    "    # Tail-safe stores\n",
    "    dK_acc *= s \n",
    "    tl.store(dV_blk, dV_acc.to(dV.type.element_ty), boundary_check=(0, 1))\n",
    "    tl.store(dK_blk, dK_acc.to(dK.type.element_ty), boundary_check=(0, 1))\n",
    "    \n",
    "\n",
    "@triton.autotune(\n",
    "    [\n",
    "        triton.Config(\n",
    "            {\"BLOCK_Q\": BLOCK_Q, \"BLOCK_KV\": BLOCK_KV, \"GROUP_N\": GROUP_N},\n",
    "            num_stages=num_stages,\n",
    "            num_warps=num_warps,\n",
    "        )\n",
    "        for BLOCK_Q in [64, 128]\n",
    "        for BLOCK_KV in [32, 64]\n",
    "        for GROUP_N in GROUP_NM_SWEEP\n",
    "        for num_stages in NUM_STAGES_SWEEP\n",
    "        for num_warps in NUM_WARPS_SWEEP\n",
    "    ],\n",
    "    key=KEY_CACHE,\n",
    ")\n",
    "@triton.jit\n",
    "def _attn_bwd_dq(\n",
    "    Q, K, V, dO, dQ, M, D,\n",
    "    # Q strides\n",
    "    sqb, sqh, sqs, sqd,\n",
    "    # K strides\n",
    "    skb, skh, sks, skd,\n",
    "    # V strides\n",
    "    svb, svh, svs, svd,\n",
    "    # dO strides\n",
    "    sob, soh, sos, sod,\n",
    "    # dK strides\n",
    "    s_dqb, s_dqh, s_dqs, s_dqd,\n",
    "    NUM_HEADS: tl.constexpr , SEQ_LEN: tl.constexpr,\n",
    "    BLOCK_Q: tl.constexpr, BLOCK_KV: tl.constexpr, \n",
    "    HEAD_DIM: tl.constexpr, DTYPE: tl.constexpr,\n",
    "    GROUP_N: tl.constexpr, softmax_scale: tl.constexpr,\n",
    "):\n",
    "    pid_bh = tl.program_id(1)\n",
    "    b = pid_bh // NUM_HEADS\n",
    "    h = pid_bh %  NUM_HEADS\n",
    "    \n",
    "    off_bh_seq = (pid_bh * SEQ_LEN).to(tl.int64)\n",
    "    M += off_bh_seq\n",
    "    D += off_bh_seq\n",
    "\n",
    "    # --- GROUP_M swizzle over Q tiles (tail-safe) ---\n",
    "    pid_q = tl.program_id(0)\n",
    "    num_tiles_m   = tl.cdiv(SEQ_LEN, BLOCK_Q)\n",
    "    group_id      = pid_q // GROUP_N\n",
    "    group_start   = group_id * GROUP_N\n",
    "    # if this CTA's group starts past the last tile, exit early\n",
    "    if group_start >= num_tiles_m:\n",
    "        return\n",
    "    tiles_in_this = tl.minimum(GROUP_N, num_tiles_m - group_start)\n",
    "    m_in_grp      = pid_q - group_start\n",
    "    m_eff         = m_in_grp % tiles_in_this\n",
    "    rot           = pid_bh % tiles_in_this\n",
    "    m_swizzled    = group_start + ((m_eff + rot) % tiles_in_this)\n",
    "\n",
    "    start_q = m_swizzled * BLOCK_Q\n",
    "    if start_q >= SEQ_LEN:\n",
    "        return\n",
    "    \n",
    "    off_bh_k  = (b * skb   + h * skh  ).to(tl.int64)\n",
    "    off_bh_v  = (b * svb   + h * svh  ).to(tl.int64)\n",
    "    off_bh_dq = (b * s_dqb + h * s_dqh).to(tl.int64)\n",
    "    off_bh_q  = (b * sqb   + h * sqh  ).to(tl.int64)\n",
    "    off_bh_do = (b * sob   + h * soh  ).to(tl.int64)\n",
    "    # ---------- block pointers ----------\n",
    "    Q_blk = tl.make_block_ptr(\n",
    "        Q + off_bh_q,(SEQ_LEN, HEAD_DIM),(sqs, sqd),(start_q, 0),(BLOCK_Q, HEAD_DIM),(1, 0),\n",
    "    )\n",
    "    dO_blk = tl.make_block_ptr(\n",
    "        dO + off_bh_do,(SEQ_LEN, HEAD_DIM),(sos, sod),(start_q, 0),(BLOCK_Q, HEAD_DIM),(1, 0),\n",
    "    )\n",
    "    K_T_blk = tl.make_block_ptr(\n",
    "        K + off_bh_k,(HEAD_DIM, SEQ_LEN),(skd, sks),(0, 0),(HEAD_DIM, BLOCK_KV),(0, 1),\n",
    "    )\n",
    "    V_T_blk = tl.make_block_ptr(\n",
    "        V + off_bh_v,(HEAD_DIM, SEQ_LEN),(svd, svs),(0, 0),(HEAD_DIM, BLOCK_KV),(0, 1),\n",
    "    )\n",
    "    dQ_blk = tl.make_block_ptr(\n",
    "        dQ + off_bh_dq,(SEQ_LEN, HEAD_DIM),(s_dqs, s_dqd),(start_q, 0),(BLOCK_Q, HEAD_DIM),(1, 0),\n",
    "    )\n",
    "\n",
    "    # ---------- indices & constants ----------\n",
    "    offs_q = start_q + tl.arange(0, BLOCK_Q)\n",
    "    offs_kv = tl.arange(0, BLOCK_KV)\n",
    "\n",
    "    # row-wise scalars\n",
    "    m  = tl.load(M + offs_q, mask=offs_q < SEQ_LEN, other=0.0)[:, None]  # [BLOCK_Q, 1]\n",
    "    Di = tl.load(D + offs_q, mask=offs_q < SEQ_LEN, other=0.0)           # [BLOCK_Q]\n",
    "    s = tl.full([1], softmax_scale, dtype=DTYPE)\n",
    "    Q_block  = tl.load(Q_blk,  boundary_check=(0, 1), padding_option=\"zero\") * s\n",
    "    dO_block = tl.load(dO_blk, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "    dQ_block = tl.zeros((BLOCK_Q, HEAD_DIM), dtype=tl.float32)\n",
    "\n",
    "    # ---------- loop over KV tiles ----------\n",
    "    num_steps = tl.cdiv(SEQ_LEN, BLOCK_KV)\n",
    "    for step in range(num_steps):\n",
    "        K_T_block = tl.load(K_T_blk, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "        V_T_block = tl.load(V_T_blk, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "        \n",
    "        start_kv = step * BLOCK_KV\n",
    "        kv_idx   = start_kv + offs_kv\n",
    "        kv_valid = kv_idx < SEQ_LEN\n",
    "        S = tl.dot(Q_block, K_T_block)                     # [BLOCK_Q, BLOCK_KV]\n",
    "        S = tl.where(kv_valid[None, :], S, -float(\"inf\"))\n",
    "        P = tl.exp(S - m)                                  # [BLOCK_Q, BLOCK_KV]\n",
    "\n",
    "        # dP = dO @ Vᵀ  (match dtypes for dot)\n",
    "        dP = tl.dot(dO_block.to(DTYPE), V_T_block.to(DTYPE)).to(tl.float32)\n",
    "        dS = (P * (dP - Di[:, None])).to(DTYPE)\n",
    "        dQ_block = tl.dot(dS, tl.trans(K_T_block.to(DTYPE)), dQ_block)\n",
    "\n",
    "        K_T_blk = tl.advance(K_T_blk, (0, BLOCK_KV))\n",
    "        V_T_blk = tl.advance(V_T_blk, (0, BLOCK_KV))\n",
    "    \n",
    "    dQ_block *= s\n",
    "    tl.store(dQ_blk, dQ_block.to(dQ.type.element_ty), boundary_check=(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "class TritonAttention(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V):\n",
    "        BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM = Q.size()\n",
    "        comp_torch = _sdpa_comp_dtype(Q)\n",
    "        comp_triton = _triton_compute_dtype(comp_torch)\n",
    "        \n",
    "        softmax_scale = 1 / (HEAD_DIM**0.5)\n",
    "        O = torch.empty_like(Q)\n",
    "\n",
    "        grid = lambda args: (\n",
    "            triton.cdiv(SEQ_LEN, args[\"BLOCK_Q\"]),\n",
    "            BATCH_SIZE * NUM_HEADS,\n",
    "        )\n",
    "        # M is the logsumexp for the backward pass, one for each query\n",
    "        M = torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN), device=Q.device, dtype=torch.float32\n",
    "        )\n",
    "        _attn_fwd[grid](\n",
    "            Q, K, V, M, O,\n",
    "            *Q.stride(), *K.stride(), *V.stride(), *O.stride(),\n",
    "            NUM_HEADS=Q.shape[1], SEQ_LEN=Q.shape[2], HEAD_DIM=HEAD_DIM, \n",
    "            softmax_scale=softmax_scale, DTYPE=comp_triton,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(Q, K, V, O, M)\n",
    "        ctx.grid = grid\n",
    "        ctx.softmax_scale = softmax_scale\n",
    "        ctx.HEAD_DIM = HEAD_DIM\n",
    "        ctx.comp_triton = comp_triton\n",
    "        return O\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dO):\n",
    "        Q, K, V, O, M = ctx.saved_tensors\n",
    "        #dO = dO.contiguous()\n",
    "        #assert dO.is_contiguous()\n",
    "        #assert Q.stride() == K.stride() == V.stride() == O.stride() == dO.stride()\n",
    "        dQ = torch.empty_like(Q)\n",
    "        dK = torch.empty_like(K)\n",
    "        dV = torch.empty_like(V)\n",
    "\n",
    "        BATCH_SIZE, NUM_HEADS, SEQ_LEN, _ = Q.size()\n",
    "\n",
    "        D = torch.empty_like(M) \n",
    "        pre_grid = lambda meta: (triton.cdiv(SEQ_LEN, meta[\"BLOCK_Q\"]),\n",
    "                         BATCH_SIZE * NUM_HEADS)\n",
    "        _attn_bwd_preprocess[pre_grid](\n",
    "            O, dO, D, \n",
    "            *O.stride(),\n",
    "            *dO.stride(),\n",
    "            NUM_HEADS=NUM_HEADS, SEQ_LEN=SEQ_LEN, HEAD_DIM=ctx.HEAD_DIM,\n",
    "        )\n",
    "        #assert torch.isnan(D).sum() == 0\n",
    "        dkdv_grid = lambda meta: (triton.cdiv(SEQ_LEN, meta[\"BLOCK_KV\"]),\n",
    "                BATCH_SIZE * NUM_HEADS)\n",
    "        # Fix KV and iterate through all the Q blocks\n",
    "        _attn_bwd_dk_dv[dkdv_grid](\n",
    "            Q, K, V, dO, dK, dV, M, D,\n",
    "            *Q.stride(), *K.stride(), *V.stride(), *dO.stride(),\n",
    "            *dK.stride(), *dV.stride(),\n",
    "            NUM_HEADS=NUM_HEADS, SEQ_LEN=SEQ_LEN, HEAD_DIM=ctx.HEAD_DIM, DTYPE=ctx.comp_triton, \n",
    "            softmax_scale=ctx.softmax_scale\n",
    "        )\n",
    "        #assert torch.isnan(dK).sum() == 0\n",
    "        #assert torch.isnan(dV).sum() == 0\n",
    "\n",
    "        dq_grid = lambda meta: (triton.cdiv(SEQ_LEN, meta[\"BLOCK_Q\"]),\n",
    "                    BATCH_SIZE * NUM_HEADS)\n",
    "        _attn_bwd_dq[dq_grid](\n",
    "            Q, K, V, dO, dQ, M, D,\n",
    "            *Q.stride(), *K.stride(), *V.stride(), *dO.stride(),\n",
    "            *dQ.stride(), \n",
    "            NUM_HEADS=NUM_HEADS, SEQ_LEN=SEQ_LEN, HEAD_DIM=ctx.HEAD_DIM, DTYPE=ctx.comp_triton,\n",
    "            softmax_scale=ctx.softmax_scale\n",
    "        )\n",
    "        #assert torch.isnan(dQ).sum() == 0\n",
    "        return dQ, dK, dV\n",
    "    \n",
    "    \n",
    "def sdpa_triton_fa(Q: Tensor, K: Tensor, V: Tensor):\n",
    "    \"\"\"ViT-S-only autograd op (single-pass forward + exact backward).\"\"\"\n",
    "    #Q = Q.contiguous()\n",
    "    #K = K.contiguous()\n",
    "    #V = V.contiguous()\n",
    "    return TritonAttention.apply(Q, K, V)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Standard Multi-head Self Attention module with QKV projection.\n",
    "\n",
    "    This module implements the standard multi-head attention mechanism used in transformers.\n",
    "    It supports both the fused attention implementation (scaled_dot_product_attention) for\n",
    "    efficiency when available, and a manual implementation otherwise. The module includes\n",
    "    options for QK normalization, attention dropout, and projection dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim:int = 384,\n",
    "            num_heads: int = 6,\n",
    "            qkv_bias: bool = False,\n",
    "            qk_norm: bool = False,\n",
    "            scale_norm: bool = False,\n",
    "            proj_bias: bool = True,\n",
    "            attn_drop: float = 0.,\n",
    "            proj_drop: float = 0.,\n",
    "            norm_layer = nn.LayerNorm,\n",
    "            device=\"cuda\",\n",
    "            dtype=torch.float32,\n",
    "            triton_kernel=True\n",
    "            ) -> None:\n",
    "        \"\"\"Initialize the Attention module.\n",
    "\n",
    "        Args:\n",
    "            dim: Input dimension of the token embeddings\n",
    "            num_heads: Number of attention heads\n",
    "            qkv_bias: Whether to use bias in the query, key, value projections\n",
    "            qk_norm: Whether to apply normalization to query and key vectors\n",
    "            proj_bias: Whether to use bias in the output projection\n",
    "            attn_drop: Dropout rate applied to the attention weights\n",
    "            proj_drop: Dropout rate applied after the output projection\n",
    "            norm_layer: Normalization layer constructor for QK normalization if enabled\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        dd = {'device': device, 'dtype': dtype}\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        if qk_norm or scale_norm:\n",
    "            assert norm_layer is not None, 'norm_layer must be provided if qk_norm or scale_norm is True'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias, **dd)\n",
    "        self.q_norm = norm_layer(self.head_dim, **dd) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim, **dd) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.norm = norm_layer(dim, **dd) if scale_norm else nn.Identity()\n",
    "        self.proj = nn.Linear(dim, dim, bias=proj_bias, **dd)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.triton_kernel = triton_kernel\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "        \n",
    "        if self.triton_kernel:\n",
    "            x = sdpa_triton_fa(q, k, v)\n",
    "        else:\n",
    "            x = F.scaled_dot_product_attention(q, k, v)\n",
    "        \n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int, mlp_ratio: float = 4.0, drop: float = 0.0):\n",
    "        super().__init__()\n",
    "        hidden = int(dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(dim, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, dim)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int = 384,\n",
    "        num_heads: int = 6,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        proj_drop: float = 0.0,\n",
    "        qkv_bias: bool = True,\n",
    "        qk_norm: bool = False,\n",
    "        scale_norm: bool = False,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        triton=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim=dim, num_heads=num_heads, qkv_bias=qkv_bias,\n",
    "            qk_norm=qk_norm, scale_norm=scale_norm,\n",
    "            attn_drop=attn_drop, proj_drop=proj_drop,\n",
    "            norm_layer=norm_layer, triton_kernel=triton\n",
    "        )\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = MLP(dim, mlp_ratio=mlp_ratio, drop=proj_drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(x) #self.norm1(x))\n",
    "        assert torch.isnan(x).sum() == 0\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ToyTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Transformer encoder using your Attention block.\n",
    "    Expects inputs shaped [B, N, C] and returns logits [B, num_classes].\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int = 384,\n",
    "        depth: int = 4,\n",
    "        num_heads: int = 6,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        num_classes: int = 1000,\n",
    "        max_len: int = 197,\n",
    "        attn_drop: float = 0.0,\n",
    "        proj_drop: float = 0.0,\n",
    "        qkv_bias: bool = True,\n",
    "        qk_norm: bool = False,\n",
    "        scale_norm: bool = False,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        cls_pool: str = \"mean\",  # \"mean\" or \"first\"\n",
    "        triton=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.cls_pool = cls_pool\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                dim=dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                attn_drop=attn_drop, proj_drop=proj_drop,\n",
    "                qkv_bias=qkv_bias, qk_norm=qk_norm, scale_norm=scale_norm,\n",
    "                norm_layer=norm_layer, triton=triton\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = norm_layer(dim)\n",
    "        self.head = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            assert torch.isnan(x).sum() == 0, f\"block {i}\"\n",
    "\n",
    "        x = self.norm(x)\n",
    "        if self.cls_pool == \"first\":\n",
    "            x = x[:, 0]                 # [B, C]\n",
    "        else:\n",
    "            x = x.mean(dim=1)           # [B, C] mean-pool over N\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3336a0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_diff 0.0 0.0\n",
      "g_diff 1.865180365712149e-08 2.765358608769475e-09\n",
      "l_diff 0.0 0.0\n",
      "g_diff 3.820164238277357e-07 1.1808817035330321e-08\n",
      "l_diff 0.0 0.0\n",
      "g_diff 7.067016667861026e-06 8.927385408696864e-08\n",
      "l_diff 0.0 0.0\n",
      "g_diff 4.4637479732045904e-05 3.429436787882878e-07\n",
      "l_diff 0.0 0.0\n",
      "g_diff 0.00734396418556571 1.0270650818711147e-05\n",
      "l_diff 0.0 0.0\n",
      "g_diff 0.18744412064552307 0.00034679876989685\n",
      "l_diff 0.015625 0.015625\n",
      "g_diff 0.007186077069491148 1.8102227841154672e-05\n",
      "l_diff 0.078125 0.078125\n",
      "g_diff 0.0013323574094101787 5.336064987204736e-06\n",
      "l_diff 0.109375 0.109375\n",
      "g_diff 9.639918425818905e-05 7.131186521291966e-07\n",
      "l_diff 0.078125 0.078125\n",
      "g_diff 0.0001267079933313653 3.625587225997151e-07\n"
     ]
    }
   ],
   "source": [
    "B, N, D = 32, 196, 384\n",
    "from copy import deepcopy\n",
    "\n",
    "model_triton = ToyTransformer(dim=D, depth=16, num_heads=6, num_classes=100).cuda()  # fp32 weights\n",
    "model_torch = deepcopy(model_triton)\n",
    "for b in model_torch.blocks:\n",
    "    b.attn.triton_kernel = False\n",
    "\n",
    "\n",
    "opt_triton = torch.optim.AdamW(model_triton.parameters(), lr=1e-3)\n",
    "opt_torch = torch.optim.AdamW(model_torch.parameters(), lr=1e-3)\n",
    "\n",
    "for i  in range(10):\n",
    "    x = torch.randn(B, N, D, device=\"cuda\")  # inputs can be bf16 or fp32\n",
    "    x_triton = x.clone().detach().requires_grad_(True)\n",
    "    x_torch = x.clone().detach().requires_grad_(True)\n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        y_triton = model_triton(x_triton)\n",
    "        y_torch = model_torch(x_torch)\n",
    "        loss_triton = y_triton.mean()\n",
    "        loss_torch = y_torch.mean()\n",
    "        l_diff = torch.abs(loss_triton - loss_torch)\n",
    "        print(\"l_diff\", l_diff.max().item(), l_diff.mean().item())\n",
    "        \n",
    "        \n",
    "    loss_triton.backward()\n",
    "    loss_torch.backward()\n",
    "    g_diff = torch.abs(x_triton.grad - x_torch.grad)\n",
    "    print(\"g_diff\", g_diff.max().item(), g_diff.mean().item())\n",
    "    \n",
    "    opt_triton.step()\n",
    "    opt_torch.step()\n",
    "    \n",
    "    opt_triton.zero_grad()\n",
    "    opt_torch.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cee967",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2619903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe: /notebooks/venvs/pt27cu118/bin/python\n",
      "Torch: 2.7.1+cu118 CUDA: 11.8\n",
      "/notebooks/venvs/pt27cu118/bin/python\n",
      "/notebooks/pytorch-image-models/timm/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "os.environ[\"COMET_API_KEY\"]=\"R7OuT6FolA02VmQRI82xDN48O\"\n",
    "os.environ[\"COMET_DISABLE_AUTO_LOGGING\"]=\"1\"\n",
    "os.environ[\"TORCHINDUCTOR_FX_GRAPH_CACHE\"]=\"1\"\n",
    "os.environ[\"TORCHINDUCTOR_AUTOGRAD_CACHE\"]=\"1\"\n",
    "os.environ[\"TRITON_PRINT_AUTOTUNING\"]=\"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "import timm\n",
    "import train as timm_train\n",
    " \n",
    "sys.path.insert(0, \"/notebooks/pytorch-image-models\")  # your fork\n",
    "\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(False)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "dynamo_config = torch._dynamo.config\n",
    "dynamo_config.compiled_autograd = True\n",
    "dynamo_config.capture_scalar_outputs = False\n",
    "dynamo_config.cache_size_limit = 512\n",
    "\n",
    "print(\"Python exe:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.version.cuda)\n",
    "print(sys.executable)                             # your venv python\n",
    "print(Path(timm.__file__).resolve())      # -> /notebooks/pytorch-image-models/timm/__init__.py\n",
    "\n",
    "with open(\"/notebooks/params_timm.yaml\", 'r', encoding='utf-8') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "507f958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m An experiment with the same configuration options is already running and will be reused.\n",
      "Training with a single process on 1 device (cuda).\n",
      "Training with a single process on 1 device (cuda).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting model to deit3_small_patch16_224\n",
      "Setting num_classes to 1000\n",
      "Setting img_size to 224\n",
      "Setting in_chans to None\n",
      "Setting dataset to hfds-disk:/notebooks/data/imagenet_1k_resized_256\n",
      "Setting data_dir to notebooks/data/imagenet_1k_resized_256\n",
      "Setting train_split to train\n",
      "Setting val_split to val\n",
      "Setting interpolation to bicubic\n",
      "Setting train_interpolation to random\n",
      "Setting crop_pct to None\n",
      "Setting batch_size to 1024\n",
      "Setting validation_batch_size to None\n",
      "Setting workers to 10\n",
      "Setting pin_mem to True\n",
      "Setting channels_last to True\n",
      "Setting amp to True\n",
      "Setting amp_dtype to bfloat16\n",
      "Setting amp_impl to native\n",
      "Setting opt to adamw\n",
      "Setting weight_decay to 0.05\n",
      "Setting lr to None\n",
      "Setting lr_base to 0.0005\n",
      "Setting lr_base_size to 1024\n",
      "Setting lr_base_scale to linear\n",
      "Setting momentum to 0.9\n",
      "Setting sched to cosine\n",
      "Setting epochs to 400\n",
      "Setting warmup_epochs to 5\n",
      "Setting warmup_lr to 1e-05\n",
      "Setting min_lr to 1e-06\n",
      "Setting lr_k_decay to 1.0\n",
      "Setting cooldown_epochs to 0\n",
      "Setting lr_cycle_limit to 1\n",
      "Setting lr_cycle_mul to 1.0\n",
      "Setting lr_cycle_decay to 0.5\n",
      "Setting lr_noise to None\n",
      "Setting aa to rand-m9-mstd0.5\n",
      "Setting hflip to 0.5\n",
      "Setting color_jitter to 0.3\n",
      "Setting reprob to 0.25\n",
      "Setting remode to pixel\n",
      "Setting mixup to 0.8\n",
      "Setting cutmix to 1.0\n",
      "Setting mixup_prob to 1.0\n",
      "Setting mixup_switch_prob to 0.5\n",
      "Setting mixup_mode to batch\n",
      "Setting smoothing to 0.0\n",
      "Setting drop to 0.0\n",
      "Setting drop_path to 0.1\n",
      "Setting bce_loss to True\n",
      "Setting bce_sum to False\n",
      "Setting bce_target_thresh to None\n",
      "Setting bce_pos_weight to None\n",
      "Setting jsd_loss to False\n",
      "Setting model_ema to True\n",
      "Setting model_ema_decay to 0.9998\n",
      "Setting model_ema_warmup to True\n",
      "Setting model_ema_force_cpu to False\n",
      "Setting pretrained to False\n",
      "Setting initial_checkpoint to \n",
      "Setting resume to True\n",
      "Setting no_resume_opt to False\n",
      "Setting checkpoint_hist to -1\n",
      "Setting experiment to deit3_s_224_v1_triton\n",
      "Setting eval_metric to top1\n",
      "Setting log_interval to 50\n",
      "Setting save_images to False\n",
      "Setting log_wandb to False\n",
      "Setting seed to 42\n",
      "Setting dist_bn to reduce\n",
      "Setting no_prefetcher to False\n",
      "Setting grad_accum_steps to 1\n",
      "Setting clip_grad to 1.0\n",
      "Setting clip_mode to norm\n",
      "Setting torchcompile to inductor\n",
      "Setting torchcompile_fullgraph to True\n",
      "Setting torchcompile_dynamic to False\n",
      "Setting comet_exp_name to timm_deit_3_triton\n",
      "Setting comet_exp_key to None\n",
      "Setting return_model to True\n",
      "Returning model\n"
     ]
    }
   ],
   "source": [
    "model = timm_train.main(cfg).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e7a7b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath(drop_prob=0.009)\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath(drop_prob=0.009)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath(drop_prob=0.018)\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath(drop_prob=0.018)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath(drop_prob=0.027)\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath(drop_prob=0.027)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath(drop_prob=0.036)\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath(drop_prob=0.036)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath(drop_prob=0.045)\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath(drop_prob=0.045)\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath(drop_prob=0.055)\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath(drop_prob=0.055)\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath(drop_prob=0.064)\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath(drop_prob=0.064)\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath(drop_prob=0.073)\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath(drop_prob=0.073)\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath(drop_prob=0.082)\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath(drop_prob=0.082)\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath(drop_prob=0.091)\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath(drop_prob=0.091)\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath(drop_prob=0.100)\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath(drop_prob=0.100)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=384, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39df86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp = torch.compile(\n",
    "                model,\n",
    "                backend=\"inductor\",\n",
    "                mode=\"max-autotune\",\n",
    "                fullgraph=True,\n",
    "                dynamic=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e2b5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = 2, 3, 224, 224\n",
    "\n",
    "opt = torch.optim.AdamW(model_comp.parameters(), lr=1e-3)\n",
    "\n",
    "for i  in range(10):\n",
    "    x = torch.randn(B, C, H, W, device=\"cuda\", dtype=torch.float32)\n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        y = model_comp(x)\n",
    "        loss = y.mean()\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()    \n",
    "    opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train: 54 [ 100/1251 (  8%)]  Loss: 0.00292 (0.00420)  Time: 0.300s, 3416.47/s  (0.581s, 1763.26/s)  LR: 4.779e-04  Data: 0.019 (0.141)\n",
    "Train: 54 [ 150/1251 ( 12%)]  Loss: 0.00472 (0.00422)  Time: 0.297s, 3452.68/s  (0.526s, 1946.67/s)  LR: 4.779e-04  Data: 0.027 (0.141)\n",
    "Train: 54 [ 200/1251 ( 16%)]  Loss: 0.00318 (0.00418)  Time: 0.300s, 3412.30/s  (0.502s, 2040.79/s)  LR: 4.779e-04  Data: 0.019 (0.144)\n",
    "Train: 54 [ 250/1251 ( 20%)]  Loss: 0.00510 (0.00414)  Time: 0.282s, 3625.44/s  (0.482s, 2125.26/s)  LR: 4.779e-04  Data: 0.013 (0.141)\n",
    "Train: 54 [ 300/1251 ( 24%)]  Loss: 0.00453 (0.00416)  Time: 0.292s, 3502.42/s  (0.470s, 2179.30/s)  LR: 4.779e-04  Data: 0.022 (0.140)\n",
    "Train: 54 [ 350/1251 ( 28%)]  Loss: 0.00467 (0.00417)  Time: 0.283s, 3619.12/s  (0.463s, 2212.44/s)  LR: 4.779e-04  Data: 0.013 (0.141)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b39db2d",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
